name: Scrape AI Tools

on:
  # ðŸ•’ Run automatically every Monday at 03:00 UTC
  schedule:
    - cron: "0 3 * * 1"

  # â–¶ï¸ Allow manual runs from the Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # ----------------------------
      # 1ï¸âƒ£ Checkout repository
      # ----------------------------
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      # ----------------------------
      # 2ï¸âƒ£ Set up Python environment
      # ----------------------------
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      # ----------------------------
      # 3ï¸âƒ£ Install dependencies
      # ----------------------------
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # ----------------------------
      # 4ï¸âƒ£ Preserve previous dataset
      # ----------------------------
      - name: Prepare previous data for diff
        run: |
          if [ -f aitools.json ]; then
            mv aitools.json aitools.json.old
          fi

      # ----------------------------
      # 5ï¸âƒ£ Run scraper (stop on failure)
      # ----------------------------
      - name: Run scraper
        run: |
          set -e  # â›” Stop if scraper fails
          python scraper.py

      # ----------------------------
      # 6ï¸âƒ£ Generate diff file
      # ----------------------------
      - name: Create diff file
        run: |
          echo "Creating diffâ€¦"

          # If no previous file exists, create an empty diff
          if [ ! -f aitools.json.old ]; then
            echo '{"added": [], "removed": [], "updated": []}' > aitools-diff.json
            exit 0
          fi

          python << 'EOF'
import json

def load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return []

old = load_json("aitools.json.old")
new = load_json("aitools.json")

# Index data by URL for easy comparison
old_map = {t.get("url"): t for t in old}
new_map = {t.get("url"): t for t in new}

# Added / Removed
added = [v for k, v in new_map.items() if k not in old_map]
removed = [v for k, v in old_map.items() if k not in new_map]

# Updated (content differences)
updated = []
for url in set(old_map.keys()) & set(new_map.keys()):
    old_item = old_map[url]
    new_item = new_map[url]
    changes = {}
    for field in new_item:
        if new_item[field] != old_item.get(field):
            changes[field] = {"old": old_item.get(field), "new": new_item[field]}
    if changes:
        updated.append({"url": url, "changes": changes})

# Save diff
diff = {
    "added": added,
    "removed": removed,
    "updated": updated
}

with open("aitools-diff.json", "w", encoding="utf-8") as f:
    json.dump(diff, f, indent=2, ensure_ascii=False)

print(f"Added: {len(added)}, Removed: {len(removed)}, Updated: {len(updated)}")

EOF

      # ----------------------------
      # 7ï¸âƒ£ Commit and push results
      # ----------------------------
      - name: Commit changes
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"

          git add aitools.json aitools-diff.json

          # Skip commit if nothing changed
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Weekly scrape update"
          git push
